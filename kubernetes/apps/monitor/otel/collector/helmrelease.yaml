---
# yaml-language-server: $schema=https://kubernetes-schemas.dmfrey.com/helm.toolkit.fluxcd.io/helmrelease_v2beta2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: otelcol-dm
spec:

  interval: 15m

  chart:
    spec:
      chart: opentelemetry-collector
      version: 0.108.0
      sourceRef:
        kind: HelmRepository
        name: opentelemetry-charts
        namespace: flux-system

  install:
    remediation:
      retries: 3

  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3

  uninstall:
    keepHistory: false

  values:

    mode: daemonset

    annotations:
      configmap.reloader.stakater.com/reload: otelcol-dm-opentelemetry-collector-agent

    extraEnvs:
      - name: K8S_NODE_IP
        valueFrom:
          fieldRef:
            fieldPath: status.hostIP

    image:
      repository: otel/opentelemetry-collector-contrib

    presets:
      # enables the k8sattributesprocessor and adds it to the traces, metrics, and logs pipelines
      kubernetesAttributes:
        enabled: true
        extractAllPodLabels: true
        extractAllPodAnnotations: true
      # enables the kubeletstatsreceiver and adds it to the metrics pipelines
      kubeletMetrics:
        enabled: false
      # Enables the filelogreceiver and adds it to the logs pipelines
      logsCollection:
        enabled: true
        includeCollectorLogs: true
        storeCheckpoints: true
      hostMetrics:
        enabled: true

    config:

      extensions:
        file_storage/otc:
          directory: /var/lib/otelcol

        health_check: {}

        zpages:

      processors:
        attributes:
          actions:
            - action:
              key: loki.attribute.labels
              value: event.domain, event.name

        batch:
          send_batch_size: 1000
          timeout: 10s

        memory_limiter:
          check_interval: 5s
          limit_percentage: 80
          spike_limit_percentage: 25

        resource:
          attributes:

            - action: insert
              key: loki.resource.labels
              value: service, service.name, service.namespace, k8s.pod.uid, k8s.pod.ip, k8s.pod.name, k8s.deployment.name, k8s.namespace.name, cluster, job, k8s.container.name, k8s.node.name

            # - action: insert
            #   key: loki.format
            #   value: json

            - action: upsert
              key: service
              from_attribute: app.kubernetes.io/name

        resourcedetection:
          # Enriches telemetry data with resource information from the host
          detectors: ["env", "system"]
          override: false

      receivers:

        kubeletstats:
          auth_type: serviceAccount
          collection_interval: 20s
          endpoint: "https://${env:K8S_NODE_IP}:10250"
          insecure_skip_verify: true

        prometheus:
          config:
            global:
              scrape_interval: 15s # Adjust this interval as needed
            scrape_configs:
              - job_name: 'prometheus'
                static_configs:
                  - targets: ['kube-prometheus-stack-prometheus:9090'] # Adjust the Prometheus address and port

        # Data sources: traces, metrics, logs
        otlp:
          protocols:
            grpc:
              include_metadata: true
              endpoint: localhost:4317
            http:
              include_metadata: true
              endpoint: localhost:4318

        zipkin:
          include_metadata: true
          endpoint: localhost:9411

      exporters:

        # Data sources: traces, metrics, logs
        # NOTE: Prior to v0.86.0 use `logging` instead of `debug`
        debug:
          verbosity: detailed

        # Data sources: traces, metrics, logs
        otlp:
          endpoint: tempo-distributor.trace.svc.cluster.local:4317
          tls:
            insecure: true
          retry_on_failure:
            enabled: true
            # max_elapsed_time: 0

        # Data sources: traces, metrics
        otlphttp/loki:
          endpoint: http://loki-write-headless.logs.svc.cluster.local:3100/otlp
          # encoding: json
          retry_on_failure:
            enabled: true
            # max_elapsed_time: 0
          sending_queue:
            storage: file_storage/otc
        otlphttp/tempo:
          endpoint: http://tempo-distributor.trace.svc.cluster.local:4318
          retry_on_failure:
            enabled: true
            # max_elapsed_time: 0

        # Data sources: metrics
        # prometheus:
        #   endpoint: localhost:8889
        #   namespace: default

        # Data sources: metrics
        prometheusremotewrite:
          endpoint: http://thanos-receive.monitor.svc.cluster.local:19291/api/v1/receive
          tls:
            insecure: true
          retry_on_failure:
            enabled: true
            # max_elapsed_time: 0

        # Data sources: traces
        # zipkin:
        #   endpoint: http://tempo-distributor.trace.svc.cluster.local:9411/api/v2/spans

      connectors:
        spanmetrics:
          histogram:
            explicit:
              buckets: [100us, 1ms, 2ms, 6ms, 10ms, 100ms, 250ms]
          dimensions:
            - name: http.method       # extract http.method attribute from span to Prometheus label http_method
              default: GET
            - name: http.status_code  # extract http.status_code attribute from span to Prometheus label http_status_code
            - name: http.route        # extract http.route attribute from span to Prometheus label http_route
          exemplars:
            enabled: true
          exclude_dimensions: ['status.code']
          dimensions_cache_size: 1000
          aggregation_temporality: "AGGREGATION_TEMPORALITY_CUMULATIVE"
          metrics_flush_interval: 15s
          metrics_expiration: 5m
          events:
            enabled: true
            dimensions:
              - name: exception.type
              - name: exception.message
          resource_metrics_key_attributes:
            - service.name
            - telemetry.sdk.language
            - telemetry.sdk.name

      service:
        extensions:
          - health_check
          - file_storage/otc
          - zpages
        pipelines:
          traces:
            receivers: [ otlp, zipkin ]
            processors: [ memory_limiter, resource, resourcedetection, batch ]
            exporters: [ otlphttp/tempo, spanmetrics ]

          metrics:
            receivers: [ prometheus, spanmetrics ]
            processors: [ memory_limiter, resource, resourcedetection, batch ]
            exporters: [ prometheusremotewrite ]

          logs:
            receivers: [ filelog ]
            processors: [ memory_limiter, resource, resourcedetection, batch ]
            exporters: [ otlphttp/loki ]

    service:
      enabled: true
      type: ClusterIP
---
# yaml-language-server: $schema=https://kubernetes-schemas.dmfrey.com/helm.toolkit.fluxcd.io/helmrelease_v2beta2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: otelcol-dp
spec:

  interval: 15m

  chart:
    spec:
      chart: opentelemetry-collector
      version: 0.108.0
      sourceRef:
        kind: HelmRepository
        name: opentelemetry-charts
        namespace: flux-system

  install:
    remediation:
      retries: 3

  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3

  uninstall:
    keepHistory: false

  values:

    mode: deployment

    replicaCount: 1

    annotations:
      configmap.reloader.stakater.com/reload: otelcol-dp-opentelemetry-collector-agent

    image:
      repository: otel/opentelemetry-collector-contrib

    presets:
      # enables the k8sclusterreceiver and adds it to the metrics pipelines
      clusterMetrics:
        enabled: true
      # enables the k8sobjectsreceiver to collect events only and adds it to the logs pipelines
      kubernetesEvents:
        enabled: true

    config:

      extensions:
        health_check: {}

        zpages:

      processors:
        batch: {}

        memory_limiter:
          check_interval: 5s
          limit_percentage: 80
          spike_limit_percentage: 25

      receivers:

        prometheus:
          config:
            global:
              scrape_interval: 15s # Adjust this interval as needed
            scrape_configs:
              - job_name: 'prometheus'
                static_configs:
                  - targets: ['kube-prometheus-stack-prometheus:9090'] # Adjust the Prometheus address and port

        # Data sources: traces, metrics, logs
        otlp:
          protocols:
            grpc:
              endpoint: localhost:4317
            http:
              endpoint: localhost:4318

        zipkin:
          endpoint: localhost:9411

      exporters:

        # Data sources: traces, metrics, logs
        # NOTE: Prior to v0.86.0 use `logging` instead of `debug`
        debug:
          verbosity: detailed

        # Data sources: traces, metrics, logs
        otlp:
          endpoint: tempo-distributor.trace.svc.cluster.local:4317
          tls:
            insecure: true
          retry_on_failure:
            enabled: true
            max_elapsed_time: 0

        # Data sources: traces, metrics
        otlphttp/loki:
          endpoint: http://loki-write-headless.logs.svc.cluster.local:3100/otlp
          retry_on_failure:
            enabled: true
            max_elapsed_time: 0
        otlphttp/tempo:
          endpoint: http://tempo-distributor.trace.svc.cluster.local:4318
          retry_on_failure:
            enabled: true
            max_elapsed_time: 0

        # Data sources: metrics
        prometheusremotewrite:
          endpoint: http://thanos-receive.monitor.svc.cluster.local:19291/api/v1/receive
          retry_on_failure:
            enabled: true
            max_elapsed_time: 0

      service:
        pipelines:
          metrics:
            receivers: [ prometheus ]
            processors: [ memory_limiter, batch ]
            exporters: [ prometheusremotewrite ]

          # logs:
          #   # receivers: [ otlp ]
          #   processors: [ memory_limiter, batch ]
          #   exporters: [ otlphttp/loki ]

    service:
      enabled: true
      type: ClusterIP
