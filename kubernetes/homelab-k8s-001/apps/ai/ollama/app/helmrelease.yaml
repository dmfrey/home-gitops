---
# yaml-language-server: $schema=https://kubernetes-schemas.dmfrey.com/source.toolkit.fluxcd.io/ocirepository_v1.json
apiVersion: source.toolkit.fluxcd.io/v1
kind: OCIRepository
metadata:
  name: ollama
spec:
  interval: 5m
  layerSelector:
    mediaType: application/vnd.cncf.helm.chart.content.v1.tar+gzip
    operation: copy
  ref:
    tag: 4.3.0
  url: oci://ghcr.io/bjw-s-labs/helm/app-template
---
# yaml-language-server: $schema=https://kubernetes-schemas.dmfrey.com/helm.toolkit.fluxcd.io/helmrelease_v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app ollama

spec:

  interval: 1h

  chartRef:
    kind: OCIRepository
    name: ollama

  maxHistory: 2

  install:
    remediation:
      retries: 3

  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3

  uninstall:
    keepHistory: false

  valuesFrom:
    - kind: ConfigMap
      name: ollama-values

  values:

    controllers:
      *app :
        type: statefulset
        replicas: 1 # 3

        annotations:
          reloader.stakater.com/auto: "true"

        containers:
          *app :
            image:
              # repository: docker.io/ollama/ollama
              # tag: 0.7.0
              # repository: ghcr.io/tyzbit/ollama-intel-gpu
              # tag: latest
              repository: intelanalytics/ipex-llm-inference-cpp-xpu
              tag: 2.3.0-SNAPSHOT

            command:
              - /bin/sh
              - -c
              - mkdir -p /llm/ollama && cd /llm/ollama && init-ollama && exec ./ollama serve

            env:
              TZ: America/New_York
              # LIBVA_DRIVER_NAME: iHD
              no_proxy: localhost,127.0.0.1
              OLLAMA_HOST: 0.0.0.0
              OLLAMA_ORIGINS: "*"
              OLLAMA_MODELS: &pvc /models
              # OLLAMA_GPU_ENABLED: "true"

              DEVICE: iGPU
              OLLAMA_INTEL_GPU: true
              OLLAMA_NUM_GPU: 999
              ZES_ENABLE_SYSMAN: 1

              # ollama-intel-gpu
              # ONEAPI_DEVICE_SELECTOR: level_zero:0
              # IPEX_LLM_NUM_CTX: 16384

            resources:
              requests:
                cpu: 200m
              limits:
                memory: 24Gi
                gpu.intel.com/i915: "1"

            securityContext:
              privileged: true

        pod:
          affinity:
            podAntiAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                - topologyKey: "kubernetes.io/hostname"
                  labelSelector:
                    matchExpressions:
                      - key: app.kubernetes.io/name
                        operator: In
                        values:
                          - "{{ .Release.Name }}"

    service:
      *app :
        controller: *app
        type: LoadBalancer
        annotations:
          # external-dns.alpha.kubernetes.io/hostname: ollama.dmfrey.com
          lbipam.cilium.io/ips: 192.168.30.241 #, ::ffff:192.168.30.241
        ports:
          http:
            port: &port 11434

    route:
      *app :
        hostnames: ["{{ .Release.Name }}.dmfrey.com"]
        parentRefs:
          - name: envoy-internal
            namespace: network
            sectionName: https
        rules:
          - backendRefs:
              - name: *app
                port: *port

    persistence:
      config:
        enabled: true
        existingClaim: ${VOLSYNC_CLAIM}
        globalMounts:
          - path: /root/.ollama

      models:
        type: nfs
        server: nas.internal
        path: /models
        globalMounts:
          - path: /models

      dri:
        type: hostPath
        hostPath: /dev/dri

      tmp:
        enabled: true
        type: emptyDir
        medium: Memory
        globalMounts:
          - path: /tmp
