---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/source.toolkit.fluxcd.io/ocirepository_v1.json
apiVersion: source.toolkit.fluxcd.io/v1
kind: OCIRepository
metadata:
  name: ollama
spec:
  interval: 5m
  layerSelector:
    mediaType: application/vnd.cncf.helm.chart.content.v1.tar+gzip
    operation: copy
  ref:
    tag: 4.3.0
  url: oci://ghcr.io/bjw-s-labs/helm/app-template
---
# yaml-language-server: $schema=https://kubernetes-schemas.dmfrey.com/helm.toolkit.fluxcd.io/helmrelease_v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app ollama

spec:

  interval: 1h

  chartRef:
    kind: OCIRepository
    name: ollama

  maxHistory: 2

  install:
    remediation:
      retries: 3

  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3

  uninstall:
    keepHistory: false

  valuesFrom:
    - kind: ConfigMap
      name: ollama-values

  values:

    controllers:
      *app :
        type: statefulset

        annotations:
          reloader.stakater.com/auto: "true"

        containers:
          *app :
            image:
              # repository: docker.io/ollama/ollama
              # tag: 0.7.0
              repository: ghcr.io/tyzbit/ollama-intel-gpu
              tag: latest

            env:
              TZ: America/New_York
              LIBVA_DRIVER_NAME: iHD
              OLLAMA_HOST: 0.0.0.0
              OLLAMA_ORIGINS: "*"
              OLLAMA_MODELS: &pvc /models
              OLLAMA_GPU_ENABLED: "true"
              # OLLAMA_GPU_TYPE: amd

              # ollama-intel-gpu
              ONEAPI_DEVICE_SELECTOR: level_zero:0
              IPEX_LLM_NUM_CTX: 16384

            resources:
              requests:
                cpu: 2000m
                memory: 16Gi
                gpu.intel.com/i915: "1"
              limits:
                memory: 16Gi
                gpu.intel.com/i915: "1"

        pod:
          affinity:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                  - matchExpressions:
                      - key: intel.feature.node.kubernetes.io/gpu
                        operator: In
                        values:
                          - "true"
          nodeSelector:
            intel.feature.node.kubernetes.io/gpu: "true"

    service:
      *app :
        controller: *app
        type: LoadBalancer
        annotations:
          # external-dns.alpha.kubernetes.io/hostname: ollama.dmfrey.com
          lbipam.cilium.io/ips: 192.168.30.241 #, ::ffff:192.168.30.241
        ports:
          http:
            port: &port 11434

    route:
      *app :
        hostnames: ["{{ .Release.Name }}.dmfrey.com"]
        parentRefs:
          - name: internal
            namespace: kube-system
            sectionName: https
        rules:
          - backendRefs:
              - name: *app
                port: *port

    persistence:
      config:
        enabled: true
        existingClaim: ${VOLSYNC_CLAIM}
        globalMounts:
          - path: /root/.ollama

      models:
        type: nfs
        server: nas.internal
        path: /models
        globalMounts:
          - path: /models

      tmp:
        enabled: true
        type: emptyDir
        medium: Memory
        globalMounts:
          - path: /tmp
