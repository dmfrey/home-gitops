---
# yaml-language-server: $schema=https://kubernetes-schemas.dmfrey.com/helm.toolkit.fluxcd.io/helmrelease_v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: kube-prometheus-stack

spec:

  interval: 30m
  timeout: 15m

  chart:
    spec:
      chart: kube-prometheus-stack
      version: 66.2.2
      sourceRef:
        kind: HelmRepository
        name: prometheus-community
        namespace: flux-system

  install:
    crds: Skip
    remediation:
      retries: 3

  upgrade:
    cleanupOnFail: true
    crds: Skip
    remediation:
      strategy: rollback
      retries: 3

  dependsOn:
    - name: prometheus-operator-crds
      namespace: monitor
    - name: openebs
      namespace: storage

  values:

    crds:
      enabled: false

    cleanPrometheusOperatorObjectNames: true

    alertmanager:
      enabled: false
      ingress:
        enabled: true
        annotations:
          external-dns.alpha.kubernetes.io/target: internal.${SECRET_DOMAIN}
        ingressClassName: internal
        hosts: ["alert-manager.${SECRET_DOMAIN}"]
        pathType: Prefix
      alertmanagerSpec:
        useExistingSecret: true
        configSecret: alertmanager-secret
        storage:
          volumeClaimTemplate:
            spec:
              storageClassName: openebs-hostpath
              resources:
                requests:
                  storage: 1Gi

    kubeApiServer:
      serviceMonitor:
        selector:
          k8s-app: kube-apiserver

    kubeScheduler:
      service:
        selector:
          k8s-app: kube-scheduler

    kubeControllerManager: &kubeControllerManager
      service:
        selector:
          k8s-app: kube-controller-manager

    kubeEtcd:
      <<: *kubeControllerManager # etcd runs on control plane nodes

    kubeProxy:
      enabled: false

    prometheus:
      ingress:
        enabled: true
        ingressClassName: internal
        annotations:
          hajimari.io/enable: "true"
          hajimari.io/icon: "simple-icons:prometheus"
          hajimari.io/appName: "Prometheus"

          external-dns.alpha.kubernetes.io/target: internal.${SECRET_DOMAIN}
        pathType: Prefix
        hosts:
          - prometheus.${SECRET_DOMAIN}

      thanosService:
        enabled: true

      thanosServiceMonitor:
        enabled: true

      prometheusSpec:
        scrapeInterval: 1m # Must match interval in Grafana Helm chart
        podMonitorSelectorNilUsesHelmValues: false
        probeSelectorNilUsesHelmValues: false
        ruleSelectorNilUsesHelmValues: false
        scrapeConfigSelectorNilUsesHelmValues: false
        serviceMonitorSelectorNilUsesHelmValues: false
        enableAdminAPI: true
        walCompression: true
        enableFeatures:
          - auto-gomemlimit
          - memory-snapshot-on-shutdown
          - new-service-discovery-manager
        retention: 14d
        retentionSize: 70GB

        # remoteWrite:
        #   - url: http://thanos-receive.monitor.svc.cluster.local:19291/api/v1/receive

        scrapeConfigSelector:
          matchLabels:
            prometheus: scrape-me

        thanos:
          image: quay.io/thanos/thanos:v0.37.0@sha256:ae949e7579589d5caf5e3cf64e045e39694d5353c2c5ffe3bffd8d7f82bfb8a6
          objectStorageConfig:
            name: thanos-secret
            key: objstore.yml
          # renovate: datasource=docker depName=quay.io/thanos/thanos
          version: "v0.37.0"

        # additionalScrapeConfigs:
        #   - job_name: minio-job
        #     metrics_path: /minio/v3/metrics/cluster
        #     static_configs:
        #       - targets: ["minio.${SECRET_DOMAIN}"]
        #   - job_name: minio-job-node
        #     metrics_path: /minio/v3/metrics/node
        #     scheme: http
        #     static_configs:
        #       - targets: ["minio.${SECRET_DOMAIN}"]
        #   - job_name: minio-job-bucket
        #     metrics_path: /minio/v3/metrics/bucket
        #     scheme: http
        #     static_configs:
        #       - targets: ["minio.${SECRET_DOMAIN}"]
        #   - job_name: minio-job-resource
        #     metrics_path: /minio/v3/metrics/resource
        #     scheme: http
        #     static_configs:
        #       - targets: ["minio.${SECRET_DOMAIN}"]

        alertingEnpoints:
          - namespace: monitor
            name: alertmanager
            port: 9093
            apiVersion: v2

        resources:
          requests:
            cpu: 100m
          limits:
            memory: 1500Mi

        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: openebs-hostpath
              resources:
                requests:
                  storage: 75Gi

    prometheus-node-exporter:
      fullnameOverride: node-exporter

      prometheus:
        monitor:
          enabled: true
          relabelings:
            - action: replace
              regex: (.*)
              replacement: $1
              sourceLabels: ["__meta_kubernetes_pod_node_name"]
              targetLabel: kubernetes_node

    kube-state-metrics:
      fullnameOverride: kube-state-metrics

      metricLabelsAllowlist:
        - pods=[*]
        - deployments=[*]
        - persistentvolumeclaims=[*]

      prometheus:
        monitor:
          enabled: true
          relabelings:
            - action: replace
              regex: (.*)
              replacement: $1
              sourceLabels: ["__meta_kubernetes_pod_node_name"]
              targetLabel: kubernetes_node

    grafana:
      enabled: false
      forceDeployDashboards: true
