---
# yaml-language-server: $schema=https://kubernetes-schemas-5e0.pages.dev/helm.toolkit.fluxcd.io/helmrelease_v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: otelcol-dm

spec:

  interval: 15m

  chart:
    spec:
      chart: opentelemetry-collector
      version: 0.117.3
      sourceRef:
        kind: HelmRepository
        name: opentelemetry-charts
        namespace: flux-system

  install:
    remediation:
      retries: 3

  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3

  uninstall:
    keepHistory: false

  values:

    mode: daemonset

    annotations:
      configmap.reloader.stakater.com/reload: otelcol-dm-opentelemetry-collector-agent

    extraEnvs:
      - name: K8S_NODE_IP
        valueFrom:
          fieldRef:
            fieldPath: status.hostIP

    image:
      repository: otel/opentelemetry-collector-contrib

    presets:
      # enables the k8sattributesprocessor and adds it to the traces, metrics, and logs pipelines
      kubernetesAttributes:
        enabled: true
        extractAllPodLabels: true
        extractAllPodAnnotations: true
      # enables the kubeletstatsreceiver and adds it to the metrics pipelines
      kubeletMetrics:
        enabled: false
      # Enables the filelogreceiver and adds it to the logs pipelines
      logsCollection:
        enabled: true
        includeCollectorLogs: true
        storeCheckpoints: true
      hostMetrics:
        enabled: true

    config:

      extensions:
        file_storage/otc:
          directory: /var/lib/otelcol

        health_check: {}

        zpages:

      processors:
        attributes:
          actions:
            - action:
              key: loki.attribute.labels
              value: event.domain, event.name

        batch:
          send_batch_size: 1000
          send_batch_max_size: 1000
          timeout: 10s

        memory_limiter:
          check_interval: 5s
          limit_percentage: 80
          spike_limit_percentage: 25

        resource:
          attributes:

            - action: insert
              key: loki.resource.labels
              value: service, service.name, service.namespace, k8s.pod.uid, k8s.pod.ip, k8s.pod.name, k8s.deployment.name, k8s.namespace.name, cluster, job, k8s.container.name, k8s.node.name

            - action: upsert
              key: service
              from_attribute: app.kubernetes.io/name

        resourcedetection:
          # Enriches telemetry data with resource information from the host
          detectors: ["env", "system"]
          override: false

        transform:
          metric_statements:
            - context: datapoint
              statements:
              - set(attributes["namespace"], resource.attributes["k8s.namespace.name"])
              - set(attributes["container"], resource.attributes["k8s.container.name"])
              - set(attributes["pod"], resource.attributes["k8s.pod.name"])

      receivers:

        kubeletstats:
          auth_type: serviceAccount
          collection_interval: 20s
          endpoint: "https://${env:K8S_NODE_IP}:10250"
          insecure_skip_verify: true

        # prometheus:
        #   config:
        #     global:
        #       scrape_interval: 15s # Adjust this interval as needed
        #       # scrape_protocols:
        #       #   - PrometheusProto
        #       #   - OpenMetricsText1.0.0
        #       #   - OpenMetricsText0.0.1
        #       #   - PrometheusText0.0.4
        #     scrape_configs:
        #       - job_name: 'otel-collector-dm'
        #         scrape_interval: 5s
        #         static_configs:
        #           - targets: ['kube-prometheus-stack-prometheus:9090'] # Adjust the Prometheus address and port

        # Data sources: traces, metrics, logs
        otlp:
          protocols:
            grpc:
              include_metadata: true
              endpoint: 0.0.0.0:4317
            http:
              include_metadata: true
              endpoint: 0.0.0.0:4318

        zipkin:
          include_metadata: true
          endpoint: 0.0.0.0:9411

      exporters:

        # Data sources: traces, metrics, logs
        # NOTE: Prior to v0.86.0 use `logging` instead of `debug`
        debug:
          verbosity: detailed

        # Data sources: traces, metrics, logs
        # otlp:
        #   endpoint: tempo-distributor.trace.svc.cluster.local:4317
        #   tls:
        #     insecure: true
        #   retry_on_failure:
        #     enabled: true
        #     # max_elapsed_time: 0

        # Data sources: traces, metrics
        otlphttp/loki:
          endpoint: http://loki-write-headless.logs.svc.cluster.local:3100/otlp
          timeout: 20s
          tls:
            insecure_skip_verify: true
          # encoding: json
          sending_queue:
            enabled: true
            num_consumers: 10
            queue_size: 10000
            storage: file_storage/otc
          retry_on_failure:
            enabled: true
            initial_interval: 1s
            max_interval: 10s
            max_elapsed_time: 300s
        otlphttp/tempo:
          endpoint: http://tempo-distributor.observability.svc.cluster.local:4318
          tls:
            insecure: true
          retry_on_failure:
            enabled: true
            # max_elapsed_time: 0

        # Data sources: metrics
        # prometheus:
        #   endpoint: kube-prometheus-stack-prometheus.observability.svc.cluster.local:9090
        #   send_timestamps: true
        #   # metric_expiration: 180m
        #   enable_open_metrics: true
        #   add_metric_suffixes: true
        #   resource_to_telemetry_conversion:
        #     enabled: true

        # Data sources: metrics
        prometheusremotewrite:
          endpoint: http://thanos-receive-ingestor-headless.observability.svc.cluster.local:19291/api/v1/receive
          external_labels:
            thanos: "true"
          tls:
            insecure: true
          retry_on_failure:
            enabled: true
            # max_elapsed_time: 0
          resource_to_telemetry_conversion:
            enabled: true

        # Data sources: traces
        # zipkin:
        #   endpoint: http://tempo-distributor.trace.svc.cluster.local:9411/api/v2/spans

      connectors:
        spanmetrics:
          histogram:
            explicit:
              buckets: [100us, 1ms, 2ms, 6ms, 10ms, 100ms, 250ms]
          dimensions:
            - name: http.method       # extract http.method attribute from span to Prometheus label http_method
              default: GET
            - name: http.status_code  # extract http.status_code attribute from span to Prometheus label http_status_code
            - name: http.route        # extract http.route attribute from span to Prometheus label http_route
          exemplars:
            enabled: true
          exclude_dimensions: ['status.code']
          dimensions_cache_size: 1000
          aggregation_temporality: "AGGREGATION_TEMPORALITY_CUMULATIVE"
          metrics_flush_interval: 15s
          metrics_expiration: 5m
          events:
            enabled: true
            dimensions:
              - name: exception.type
              - name: exception.message
          resource_metrics_key_attributes:
            - service.name
            - telemetry.sdk.language
            - telemetry.sdk.name

      service:
        extensions:
          - health_check
          - file_storage/otc
          - zpages

        pipelines:
          traces:
            receivers: [ otlp, zipkin ]
            processors: [ ]   # memory_limiter, resource, resourcedetection, batch
            exporters: [ otlphttp/tempo ] # , spanmetrics

          metrics:
            receivers: [ otlp ] # , spanmetrics
            processors: [ memory_limiter, resource, resourcedetection, transform, batch ] # memory_limiter, resource, resourcedetection, transform, batch
            exporters: [ prometheusremotewrite ]

          logs:
            receivers: [ otlp, filelog ]
            processors: [ memory_limiter, resource, resourcedetection, batch ]
            exporters: [ otlphttp/loki ]

        # telemetry:
        #   logs:
        #     level: "info"

    service:
      enabled: true
      type: ClusterIP

---
# yaml-language-server: $schema=https://kubernetes-schemas-5e0.pages.dev/helm.toolkit.fluxcd.io/helmrelease_v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: otelcol-dp
spec:

  interval: 15m

  chart:
    spec:
      chart: opentelemetry-collector
      version: 0.117.3
      sourceRef:
        kind: HelmRepository
        name: opentelemetry-charts
        namespace: flux-system

  install:
    remediation:
      retries: 3

  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3

  uninstall:
    keepHistory: false

  values:

    mode: deployment

    replicaCount: 1

    annotations:
      configmap.reloader.stakater.com/reload: otelcol-dp-opentelemetry-collector-agent

    image:
      repository: otel/opentelemetry-collector-contrib

    presets:
      # enables the k8sclusterreceiver and adds it to the metrics pipelines
      clusterMetrics:
        enabled: true
      # enables the k8sobjectsreceiver to collect events only and adds it to the logs pipelines
      kubernetesEvents:
        enabled: true

    config:

      extensions:
        # file_storage/otc:
          # directory: /var/lib/otelcol

        health_check: {}

        zpages:

      processors:
        attributes:
          actions:
            - action:
              key: loki.attribute.labels
              value: event.domain, event.name

        batch:
          send_batch_size: 1000
          send_batch_max_size: 1000
          timeout: 10s

        memory_limiter:
          check_interval: 5s
          limit_percentage: 80
          spike_limit_percentage: 25

        resource:
          attributes:

            - action: insert
              key: loki.resource.labels
              value: service, service.name, service.namespace, k8s.pod.uid, k8s.pod.ip, k8s.pod.name, k8s.deployment.name, k8s.namespace.name, cluster, job, k8s.container.name, k8s.node.name

            - action: upsert
              key: service
              from_attribute: app.kubernetes.io/name

        resourcedetection:
          # Enriches telemetry data with resource information from the host
          detectors: ["env", "system"]
          override: false

        transform:
          metric_statements:
            - context: datapoint
              statements:
              - set(attributes["namespace"], resource.attributes["k8s.namespace.name"])
              - set(attributes["container"], resource.attributes["k8s.container.name"])
              - set(attributes["pod"], resource.attributes["k8s.pod.name"])

      receivers:

        # prometheus:
        #   config:
        #     global:
        #       scrape_interval: 15s # Adjust this interval as needed
        #       scrape_protocols:
        #         - PrometheusProto
        #         - OpenMetricsText1.0.0
        #         - OpenMetricsText0.0.1
        #         - PrometheusText0.0.4
        #     scrape_configs:
        #       - job_name: 'otel-collector-dp'
        #         scrape_interval: 5s
        #         static_configs:
        #           - targets: ['kube-prometheus-stack-prometheus:9090'] # Adjust the Prometheus address and port

        # Data sources: traces, metrics, logs
        otlp:
          protocols:
            grpc:
              endpoint: 0.0.0.0:4317
            http:
              endpoint: 0.0.0.0:4318

        # zipkin:
        #   endpoint: 0.0.0.0:9411

      exporters:

        # Data sources: traces, metrics, logs
        # NOTE: Prior to v0.86.0 use `logging` instead of `debug`
        # debug:
        #   verbosity: detailed

        # Data sources: traces, metrics, logs
        # otlp:
        #   endpoint: tempo-distributor.trace.svc.cluster.local:4317
        #   tls:
        #     insecure: true
        #   retry_on_failure:
        #     enabled: true
        #     max_elapsed_time: 0

        # Data sources: traces, metrics
        otlphttp/loki:
          endpoint: http://loki-write-headless.logs.svc.cluster.local:3100/otlp
          timeout: 20s
          tls:
            insecure_skip_verify: true
          # encoding: json
          sending_queue:
            enabled: true
            num_consumers: 10
            queue_size: 10000
            # storage: file_storage/otc
          retry_on_failure:
            enabled: true
            initial_interval: 1s
            max_interval: 10s
            max_elapsed_time: 300s
        # otlphttp/tempo:
        #   endpoint: http://tempo-distributor.trace.svc.cluster.local:4318
        #   retry_on_failure:
        #     enabled: true
        #     max_elapsed_time: 0

        # Data sources: metrics
        # prometheus:
        #   endpoint: kube-prometheus-stack-prometheus.observability.svc.cluster.local:9090
        #   send_timestamps: true
        #   # metric_expiration: 180m
        #   enable_open_metrics: true
        #   add_metric_suffixes: true
        #   resource_to_telemetry_conversion:
        #     enabled: true

        # Data sources: metrics
        prometheusremotewrite:
          endpoint: http://thanos-receive-ingestor-headless.observability.svc.cluster.local:19291/api/v1/receive
          external_labels:
            receive: "true"
          tls:
            insecure: true
          retry_on_failure:
            enabled: true
            max_elapsed_time: 0
          resource_to_telemetry_conversion:
            enabled: true

      service:
        extensions:
          - health_check
          # - file_storage/otc
          - zpages

        pipelines:
          metrics:
            receivers: [ otlp ]
            processors: [ memory_limiter, resource, resourcedetection, transform, batch ]   # memory_limiter, resource, resourcedetection, transform, batch
            exporters: [ prometheusremotewrite ]

          logs:
            receivers: [ otlp ]
            processors: [ memory_limiter, resource, resourcedetection, transform, batch ]
            exporters: [ otlphttp/loki ]

    service:
      enabled: true
      type: ClusterIP
